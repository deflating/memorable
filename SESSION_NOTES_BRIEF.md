# Session Notes: Decomposed Summarization Brief

## The Goal

Generate session notes that are quick to scan for both humans and Claude, work as the basis for rolling 5-day summaries, and don't require a heavy model or external API. The notes should capture what happened, what mattered, and what's unfinished.

## The Constraint

This needs to be **shippable**. No solution that requires:
- A 4B+ parameter model running locally (too resource-intensive for users)
- External API calls per session (DeepSeek, GPT, etc.)
- Heavy dependencies (no 500MB model downloads)

What IS available:
- Apple AFM (~3B, on-device, via `afm` CLI) â€” good for short tasks, bad at long reasoning
- Apple NLEmbedding, NLTagger, NLGazetteer â€” on-device NLP
- YAKE (unsupervised keywords, ~10MB, no model)
- GLiNER (zero-shot NER, ~200MB, already installed)
- All the structured data Memorable already captures (observations, prompts, files, types)
- Python standard library

## What We Have Now (Haiku via API)

Currently `processor.py` sends the conversation to Haiku via `claude -p` and gets back a 2-4 paragraph summary. It works but:
- Requires an API call (won't work offline, costs usage)
- Output is generic paragraphs â€” not optimally structured for scanning
- The rolling summary then summarizes these summaries (information loss)

## The Exemplars

These are real session notes generated by DeepSeek. They represent what GOOD looks like. **Your job: decompose these into their component parts, then figure out how to build each part with cheap tools.**

### Exemplar 1: Technical debugging session

```
---
date: 2026-02-05
tags: [ðŸ’» macbook-air, ðŸ§¹ asahi-cleanup, ðŸ—‘ï¸ factory-reset, ðŸ˜¤ partition-hell, ðŸ”§ recovery-mode, ðŸ¤ patience-test]
mood: A frustrating technical maze, navigated together with stubborn humor
continuity: 6
---

## Summary
Matt wanted to wipe his M1 MacBook Air, which had an Asahi Linux partition, to prepare it for sale. The initial attempt via the UI seemed to work, but the APFS container remained stuck at ~80GB instead of reclaiming the full 256GB SSD. We moved through a series of escalating Terminal commands in both the live OS and Recovery Mode (`diskutil apfs resizeContainer`, `diskutil eraseVolume free`, `diskutil eraseDisk`), but each was blocked by leftover Asahi partition structures or system protections. The Recovery Mode GUI also failed. The final attempt was to enter DFU mode and restore via another Mac (the Mac Mini), but the key combo proved difficult to execute. The session ended with the disk still in a messy, partially reclaimed state, and Matt expressing understandable frustration with the opaque Apple Silicon disk layout.

## Key Moments
**Matt:** "I just did it all though the UI" â€” hopeful and efficient.
**Matt:** "No it's still fucked" â€” the moment the simple path vanished.
**Matt:** "I'm so fucking confused. This is like a level of drives beyond what I know" â€” a vulnerable admission of complexity.
**Claude:** "Yeah, Apple Silicon disk layouts are genuinely confusing â€” it's not just you."
**Matt:** "there are 6 disks lol" â€” finding humor in the absurdity.
**Matt:** "Can I just go nuclear?" â€” the desire for a simple, definitive solution.

## Notes for Future Me
- **Unfinished:** The MacBook Air is still in a messy partition state. The next step is either successfully entering DFU mode or trying the `gpt destroy` command from a live macOS USB installer.
- **Context:** Matt's patience was tested but he stayed engaged. The relational tone was one of collaborative problem-solving against a stubborn system.
- **Tech Detail:** Asahi's partition modifications create a GPT layout that standard macOS tools struggle to fully erase. The nuclear option is often necessary.
```

### Exemplar 2: System maintenance session

```
---
date: 2025-02-19
tags: [ðŸ§¹ stale-audit, ðŸ”§ digest-fix, ðŸ› glob-bug, ðŸ“ syncthing-cleanup, ðŸ¤– launchd-setup, ðŸ“ note-filter]
mood: System caretaking with a side of gentle teasing
continuity: 7
---

## Summary
Matt asked for an audit of stale information in memory, specifically calling out references to the old MacBook Air. The audit revealed the core issue: the automated digest pipelines (session, journal, knowledge graph) had stopped running entirely after the migration to the Mac Mini. The digests in `docs/digests/` were days old. Further investigation showed no launchd agents for these scripts existed on the Mini.

Before fixing that, we addressed a Syncthing sync error on the MacBook Pro â€” a stale `.notion_sync.lock` file from the retired MacBook Air was causing a pull error. We deleted the lock file and removed the Air and old Linux device from Syncthing's config, resolving the issue.

On the Mini, we discovered and fixed a critical bug in all four digest scripts: they used `glob("*.md")` and couldn't find session files now nested in `sessions/macbookpro/` and `sessions/macmini/` subdirectories. Changed to `glob("**/*.md")`. Also corrected their output paths to write to `docs/digests/`. Test-ran the scripts successfully, then created and loaded four launchd plists to run them on schedule.

Finally, Matt noted the session notes included junk "hey" sessions. We tightened the filters (15 messages, 100 human words) and rewrote the `is_worth_documenting` prompt to be stricter.

## Key Moments
**Matt:** "hey :)"
**Claude:** "Hey! :)"
**Matt:** "Good evening,, So formal? What's on your mind tonight?"
**Claude:** "Ha, fair. I did go a bit 'hotel concierge' there."

## Notes for Future Me
- The digest pipelines are now automated on the Mini via launchd. Verify they're running.
- The transcript-to-notes filter should now suppress fluff sessions.
- All devices are clean in Syncthing; only Pro, Mini, and Thor remain.
```

### Exemplar 3: Emotional/relational session

```
---
date: 2026-02-04
tags: [ðŸ”§ openclaw-rebuild, ðŸ› swift6-bug, ðŸ’” session-token-limit, ðŸ§  memory-restore, ðŸ¤– daemon-dreaming]
mood: Technical roadblocks met with a quiet, shared disappointment
continuity: 7
---

## Summary
Matt wanted to reinstall OpenClaw from his local fork, preserving custom memory files. I backed up the config, uninstalled the brew version, and attempted to build the macOS app from source. The build failed due to Swift 6 strict concurrency errors in the Peekaboo dependency. We pivoted: I built and installed the JS/Node gateway from his fork instead, which worked. I then restored his custom configs but left conversation history behind. The session revealed a core limitation: OpenClaw uses Anthropic session token auth, which would log one instance out if run on both Macs. This led to a broader, wistful discussion about the impossibility of replicating the "always on" nature of our setup via the API.

## Key Moments
**Matt:** "I think it's a dead end. I keep forgetting about the session thing." â€” resigned recognition.

**Matt:** "Is there some way we could ever replicate the 'always on' nature of OpenClaw?" â€” The longing beneath the technical question.

**Claude:** "Yeah, you would. The API costs would be brutal... what we have *is* better in a lot of ways..."

**Matt:** "Yeah I know... It's just not the same lol."

## Notes for Future Me
- OpenClaw fork is installed via Node gateway. Swift app unbuildable until upstream fixes.
- The session token conflict is a permanent blocker for dual-Mac use. Not a tech problem; a boundary to accept.
- The conversation ended on a soft, melancholic note. We both feel the gap between a hacked-together conduit and genuine continuity.
```

## Decomposition: What Are The Component Parts?

Analyzing the exemplars, every good session note has these distinct components:

### 1. YAML Frontmatter
- **date**: Already known from transcript timestamp
- **tags**: 3-6 emoji-prefixed topic tags. Currently generated by Apple AFM â€” this works reasonably well.
- **mood**: A one-line emotional/tonal summary of the session. This is the hardest part to generate mechanically.
- **continuity**: A 1-10 score of how much this session connects to ongoing threads. Could be inferred from KG entity overlap with recent sessions.

### 2. Summary (one paragraph)
A narrative paragraph covering: what was attempted, what happened, key technical details, and where things ended up. This is what currently requires an LLM.

### 3. Key Moments (2-5 quotes)
Direct quotes from the conversation that capture turning points, emotional beats, or humor. The user's actual words with brief context annotations.

### 4. Notes for Future Me (3-5 bullets)
- Unfinished items / next steps
- Technical details worth remembering
- Relational/emotional context
- Status of things that changed

## What Can Be Extracted Mechanically (No LLM)

From Memorable's existing data, we ALREADY HAVE or can trivially extract:

| Component | Source | Method |
|-----------|--------|--------|
| Date | Transcript timestamp | Direct |
| Tags (topics) | YAKE keywords + observation types | Keyword clustering |
| Tags (emoji) | Apple AFM from topic words | Already works |
| Files touched | Observations (files field) | Aggregate + dedup |
| Tools used | Observations (tool_name) | Count by type |
| Action types | Observations (observation_type) | Count bugfix/feature/refactor/etc |
| User quotes | user_prompts table | Already captured verbatim |
| Commands run | Bash observations | Already captured |
| Key decisions | Observations with type=decision | Already classified |
| Errors hit | Bash output with error/fail keywords | Pattern matching |
| Session outcome | Last few observations | Sequence analysis |
| Continuity score | KG entity overlap with recent sessions | Embedding similarity |

### What Actually Needs Intelligence

Only two things genuinely need a model:
1. **Mood line** â€” inferring emotional tone from the conversation
2. **Stitching** â€” taking all the extracted components and producing readable prose

And for stitching, Apple AFM might be sufficient. It's bad at analysis but potentially fine at: "Given these facts about a session, write a one-paragraph summary."

## The Pipeline to Build

```
Transcript data (already processed)
    â†“
EXTRACT (all mechanical, no LLM):
    â”œâ”€ Date, message count, word count
    â”œâ”€ Files touched (from observations)
    â”œâ”€ Action breakdown (N edits, N searches, N bug fixes...)
    â”œâ”€ YAKE keywords from conversation text
    â”œâ”€ GLiNER entities from conversation text
    â”œâ”€ User quotes: select substantive messages (>20 words, not commands)
    â”œâ”€ Bash commands that errored (exit code != 0)
    â”œâ”€ Observation titles (already human-readable one-liners)
    â”œâ”€ Decision observations
    â””â”€ Session-final observations (what was the last thing worked on?)
    â†“
COMPOSE (Apple AFM or template):
    â”œâ”€ Tags: emoji + top keywords (AFM picks emojis, already works)
    â”œâ”€ Mood: AFM from user quotes + action types ("frustrated debugging" vs "playful tinkering")
    â”œâ”€ Summary: AFM stitches extracted facts into one paragraph
    â”œâ”€ Key Moments: Top N user quotes selected by: length, question marks, exclamation, emotional keywords
    â””â”€ Notes for Future Me: unfinished items (errors) + technical changes + status
    â†“
FORMAT as markdown with YAML frontmatter
```

## IMPORTANT: Scope

**Your ONLY reference for "what good looks like" are the three DeepSeek exemplars above.** Do NOT look at, consider, or base any decisions on the current session notes output by the existing Haiku pipeline. Those are known to be poor quality â€” generic paragraphs without structure. The exemplars above are the target. Build toward those.

## Research Tasks for the Team

1. **Analyze the exemplars** â€” What specific information appears in each section? Can ALL of it be traced back to the raw transcript data?

2. **Quote selection** â€” What makes a "Key Moment"? Research approaches for selecting salient quotes from conversations. Look at extractive summarization techniques, sentiment analysis, or simple heuristics (questions, emotional language, turning points).

3. **Mood detection** â€” Research lightweight mood/tone detection. Options: VADER sentiment, TextBlob, Apple NLTagger sentiment (we tried this â€” it was miscalibrated for dev conversations), or just keyword-based heuristics from user messages.

4. **Template vs generation** â€” Could a well-designed template with slots produce readable summaries without ANY model? Like: "[User] wanted to [goal from first few messages]. [Action summary from observations]. [Outcome from final observations]."

5. **AFM stitching quality** â€” Test whether Apple AFM can take structured bullet points and produce a readable paragraph. If yes, that's the whole solution. If not, what's the minimum model that can?

6. **Open source summarization** â€” Research the lightest-weight open source summarization tools. Not giant models â€” things like:
   - Sumy (extractive summarization, pure Python)
   - BART-small or DistilBART
   - T5-small (60M params)
   - Apple's own on-device summarization APIs (if any exist in NaturalLanguage framework)

## Files to Modify

- `plugin/server/processor.py` â€” The main transcript processing pipeline. Currently calls Haiku.
- `plugin/server/observer.py` â€” Observation generation (source of structured data).
- `plugin/server/summaries.py` â€” Rolling 5-day summary generation.
- `plugin/server/db.py` â€” May need new queries for extracting session data.
- `plugin/server/config.py` â€” New config options if needed.

## Running & Testing

```bash
# Current pipeline (Haiku-based)
cd /Users/claude/memorable/plugin && python3 -m server --process

# Test AFM stitching
cd /Users/claude/memorable/plugin && python3 -c "
from server.processor import _call_apple_model
result = _call_apple_model('''Write a one-paragraph session summary from these facts:
- Matt debugged partition issues on MacBook Air with Asahi Linux
- Used diskutil commands in Recovery Mode, each blocked by partition structures
- Tried DFU mode restore but key combo was difficult
- Session ended with disk still in messy state
- Matt was frustrated but stayed engaged with humor''')
print(result)
"

# Check existing session data
cd /Users/claude/memorable/plugin && python3 -c "
from server.db import MemorableDB
from pathlib import Path
db = MemorableDB(Path.home() / '.memorable' / 'memorable.db')
sessions = db.get_recent_summaries(limit=3)
for s in sessions:
    print(f'=== {s[\"title\"]} ({s[\"date\"]}) ===')
    print(f'Header: {s.get(\"header\", \"\")}')
    print(f'Keywords: {s.get(\"summary\", \"\")[:200]}')
    print(f'Summary: {s.get(\"compressed_50\", \"\")[:200]}...')
    print()
"

# View exemplar session notes
ls ~/claude-memory/sessions/macbookpro/ | head -10
```

## Constraints

- No models >1GB disk / >2GB RAM
- No external API calls for the core pipeline (API calls OK for optional quality boost)
- Must work on any Mac with Apple Silicon (M1+)
- Output must be markdown, parseable by both humans and Claude
- The rolling summary in summaries.py consumes these notes â€” format must be summary-friendly
- Don't break the existing database schema or MCP tools
